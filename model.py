'''
my proj


'''
'''
import numpy  as np
import pandas as pd
import polars as pl
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly._subplots import make_subplots

import ucimlrepo
import numpy as np
import pandas as pd
import plotly.express as px
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
'''
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import ucimlrepo
from ucimlrepo import fetch_ucirepo 
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE, SelectKBest, chi2
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score
import joblib
from sklearn.pipeline import Pipeline


# ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Heart Disease Ù…Ù† UCI
heart_disease = fetch_ucirepo(id=45) 

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù€ DataFrame
df = heart_disease.data.original
df['target'] = df['num'].apply(lambda x: 1 if x > 0 else 0)
df.drop(columns='num', inplace=True)


# 2. ÙØµÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø£Ø¹Ù…Ø¯Ø© Ø±Ù‚Ù…ÙŠØ© ÙˆÙ†ØµÙŠØ©
# Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ target Ù…Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
df_features = df.drop(columns='target')

# ÙØµÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© ÙˆØ§Ù„Ù†ØµÙŠØ© Ø¨Ø¯ÙˆÙ† target
numeric_features = df_features.select_dtypes(include=['int64', 'float64']).columns
categorical_features = df_features.select_dtypes(include=['object']).columns


# Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

# Ø¨Ù†Ø§Ø¡ preprocessor Ø¨Ø¯ÙˆÙ† target
preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
])

# ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù€ preprocessor Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† target)
preprocessor.fit(df.drop(columns='target'))


# 5. ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
X_processed = preprocessor.fit_transform(df.drop(columns='target'))
#########################################################################

# --- 1. EDA ---
# Ø±Ø³Ù… Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ù„ÙƒÙ„ Ø¹Ù…ÙˆØ¯ Ø±Ù‚Ù…ÙŠ
df.hist(figsize=(12, 10))
plt.tight_layout()
plt.show()

# Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ø¨ÙŠÙ† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

# --- 2. PCA ---
# Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù†ØµÙŠØ© Ø¥Ù† ÙˆØ¬Ø¯Øª
# ÙÙ‚Ø· Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ©
df_numeric = df.select_dtypes(include=['int64', 'float64'])

# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù†Ø§Ù‚ØµØ©
imputer = SimpleImputer(strategy='mean')
df_imputed = imputer.fit_transform(df_numeric)

# Standardization
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_imputed)



# ØªÙ†ÙÙŠØ° PCA
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Ø±Ø³Ù… Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ¨Ø§ÙŠÙ† Ø§Ù„Ù…Ø­ØªÙØ¸ Ø¨Ù‡Ø§
plt.figure(figsize=(8,5))
plt.plot(range(1, len(pca.explained_variance_ratio_)+1), 
         pca.explained_variance_ratio_.cumsum(), marker='o')
plt.xlabel("Number of Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("PCA - Variance Retained")
plt.grid(True)
plt.show()
###############################################################
# Ø¨Ù†ÙØªØ±Ø¶ Ø¥Ù† Ø§Ù„Ù…ØªØºÙŠØ± Ø§Ù„Ù‡Ø¯Ù Ø§Ø³Ù…Ù‡ 'target' (ØºÙŠÙ‘Ø± Ø§Ù„Ø§Ø³Ù… Ù„Ùˆ Ù…Ø®ØªÙ„Ù)
target_col = 'target' if 'target' in df.columns else df.columns[-1]

# Ù†Ø¬Ù‡Ø² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ù…Ù† ØºÙŠØ± OneHot Ù‡Ù†Ø§ - Ø¹Ù„Ø´Ø§Ù† Chi-Square)
X = df.drop(columns=[target_col])
y = df[target_col]

# --- 1. Feature Importance Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Random Forest ---
rf = RandomForestClassifier(random_state=42)
rf.fit(X.select_dtypes(include=['int64', 'float64']), y)

importances = rf.feature_importances_
feature_names = X.select_dtypes(include=['int64', 'float64']).columns

# Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
feat_importance = pd.Series(importances, index=feature_names).sort_values(ascending=False)
plt.figure(figsize=(8,6))
feat_importance.plot(kind='bar')
plt.title("Feature Importance - Random Forest")
plt.ylabel("Importance")
plt.grid(True)
plt.show()

# --- 2. Recursive Feature Elimination (RFE) ---
rfe = RFE(estimator=RandomForestClassifier(random_state=42), n_features_to_select=5)
rfe.fit(X.select_dtypes(include=['int64', 'float64']), y)

selected_features_rfe = feature_names[rfe.support_]
print(" RFE:", list(selected_features_rfe))

# --- 3. Chi-Square Test ---
# Ù†Ø¬Ù‡Ø² Ø¨ÙŠØ§Ù†Ø§Øª ØªØµÙ†ÙŠÙÙŠØ© + Ù‡Ø¯Ù
X_cat = X.select_dtypes(include=['int64']).copy()
X_cat = X_cat.fillna(0)  # Ù†ØªØ£ÙƒØ¯ Ù…ÙÙŠØ´ Ù‚ÙŠÙ… Ù†Ø§Ù‚ØµØ©

chi2_selector = SelectKBest(chi2, k=5)
chi2_selector.fit(X_cat, y)

chi2_features = X_cat.columns[chi2_selector.get_support()]
print("Chi-Square:", list(chi2_features))
###################################################
df_clean = df.copy()
imputer = SimpleImputer(strategy='most_frequent')
df_clean[df_clean.columns] = imputer.fit_transform(df_clean)

# --- Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---
X = df_clean.drop(columns='target')
y = df_clean['target']

# Ù„Ùˆ ÙÙŠ Ø¨ÙŠØ§Ù†Ø§Øª Ù†ØµÙŠØ©ØŒ Ù†Ø­ÙˆÙ„Ù‡Ø§ OneHot
X = pd.get_dummies(X, drop_first=True)

# ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ---
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "SVM": SVC(probability=True)
}

# --- ØªØ¯Ø±ÙŠØ¨ ÙˆØªÙ‚ÙŠÙŠÙ… ---
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]

    print(f"\nğŸ“Œ Model: {name}")
    print(classification_report(y_test, y_pred))
    print(f"AUC Score: {roc_auc_score(y_test, y_prob):.2f}")

    # Ø±Ø³Ù… ROC
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc_score(y_test, y_prob):.2f})")

# Ø±Ø³Ù… ROC Curve Ù„ÙƒÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
plt.plot([0,1], [0,1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Comparison")
plt.legend()
plt.grid(True)
plt.show()
#####################################################################

# Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
X_cluster = df_clean.drop(columns='target')

# ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_cluster)

# ØªØ¬Ø±Ø¨Ø© Ø£ÙƒØ«Ø± Ù…Ù† K
inertia = []
K_range = range(1, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)

# Ø±Ø³Ù… Elbow Curve
plt.figure(figsize=(8,5))
plt.plot(K_range, inertia, marker='o')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Inertia')
plt.title('Elbow Method - KMeans')
plt.grid(True)
plt.show()

kmeans = KMeans(n_clusters=2, random_state=42)
clusters_kmeans = kmeans.fit_predict(X_scaled)

# Ù†Ø¶ÙŠÙ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª
df_clean['kmeans_cluster'] = clusters_kmeans

# Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ± Ø¨Ø§Ù„Ù€ target
print("\nğŸ“Š K-Means Cluster vs Actual Target:\n")
print(pd.crosstab(df_clean['kmeans_cluster'], df_clean['target']))


linked = linkage(X_scaled, method='ward')

plt.figure(figsize=(12, 6))
dendrogram(linked, truncate_mode='level', p=5)
plt.title("Hierarchical Clustering Dendrogram")
plt.xlabel("Samples")
plt.ylabel("Distance")
plt.grid(True)
plt.show()

# ØªÙ†ÙÙŠØ° Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±
hc = AgglomerativeClustering(n_clusters=2)
clusters_hc = hc.fit_predict(X_scaled)
df_clean['hc_cluster'] = clusters_hc

# Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ± Ø¨Ù€ target
print("\nğŸ“Š Hierarchical Cluster vs Actual Target:\n")
print(pd.crosstab(df_clean['hc_cluster'], df_clean['target']))
#############################################################################

# --- ØªØ­Ø¯ÙŠØ¯ Ù…ÙˆØ¯ÙŠÙ„ ---
model = RandomForestClassifier(random_state=42)

# --- ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø´Ø¨ÙƒØ© ---
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
}

# --- ØªÙ†ÙÙŠØ° Randomized Search ---
random_search = RandomizedSearchCV(model, param_distributions=param_grid,
                                   n_iter=10, cv=5, scoring='f1', n_jobs=-1, random_state=42)
random_search.fit(X_train, y_train)

# --- Ø£ÙØ¶Ù„ Ù…ÙˆØ¯ÙŠÙ„ ---
best_model = random_search.best_estimator_

print("\nâœ… best parameters")
print(random_search.best_params_)

# --- ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¹Ù„Ù‰ test set ---

y_pred = best_model.predict(X_test)
y_prob = best_model.predict_proba(X_test)[:, 1]

print("\nğŸ“Š Evaluation of Tuned Model:\n")
print(classification_report(y_test, y_pred))
print(f"AUC Score: {roc_auc_score(y_test, y_prob):.2f}")

###################################################################

# Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ù…ÙˆØ¯ÙŠÙ„ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙˆÙ„ÙŠÙ
joblib.dump(best_model, 'final_model.pkl')
print("âœ… Model saved as final_model.pkl")

# Ø¥Ù†Ø´Ø§Ø¡ preprocessor
preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
])

# ğŸ‘‡ Ù‡Ù†Ø§ ØªØ¹Ù…Ù„ fit Ø¨Ø¯ÙˆÙ† target
preprocessor.fit(df.drop(columns='target'))

full_pipeline = Pipeline(steps=[
    ('preprocessing', preprocessor),
    ('classifier', best_model)
])
joblib.dump(full_pipeline, 'model_pipeline.pkl')
print("âœ… Full pipeline saved as model_pipeline.pkl")

'''
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­ÙÙˆØ¸
loaded_model = joblib.load('final_model.pkl')

# Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤
prediction = loaded_model.predict(X_test)
'''